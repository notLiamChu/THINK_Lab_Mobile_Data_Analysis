{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Kernel Experiments for Mobility Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we apply predictive models on human mobility data. Specifically, we are interested in predicting a given person's location using features from their dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Loading & Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats \n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import warnings\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.constraints.constraints import Interval, Positive\n",
    "from gpytorch.kernels.kernel import Kernel\n",
    "from gpytorch.priors.prior import Prior\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>leaving_datetime</th>\n",
       "      <th>cluster</th>\n",
       "      <th>dist</th>\n",
       "      <th>last_one</th>\n",
       "      <th>last_two</th>\n",
       "      <th>last_three</th>\n",
       "      <th>activity_duration</th>\n",
       "      <th>last_three_activity_duration</th>\n",
       "      <th>last_three_start_time</th>\n",
       "      <th>last_three_end_time</th>\n",
       "      <th>last_three_lat</th>\n",
       "      <th>last_three_lng</th>\n",
       "      <th>last_three_dist</th>\n",
       "      <th>last_dist</th>\n",
       "      <th>second_last_dist</th>\n",
       "      <th>third_last_dist</th>\n",
       "      <th>last_start_time</th>\n",
       "      <th>second_last_start_time</th>\n",
       "      <th>third_last_start_time</th>\n",
       "      <th>last_end_time</th>\n",
       "      <th>second_last_end_time</th>\n",
       "      <th>third_last_end_time</th>\n",
       "      <th>last_activity_duration</th>\n",
       "      <th>second_last_activity_duration</th>\n",
       "      <th>third_last_activity_duration</th>\n",
       "      <th>leaving_hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekday</th>\n",
       "      <th>third_last_one</th>\n",
       "      <th>second_last_one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-31 14:49:22</td>\n",
       "      <td>2019-12-31 15:58:02</td>\n",
       "      <td>0</td>\n",
       "      <td>58813.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>69</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>(NaT, NaT, NaT)</td>\n",
       "      <td>(NaT, NaT, NaT)</td>\n",
       "      <td>(nan, nan, nan)</td>\n",
       "      <td>(nan, nan, nan)</td>\n",
       "      <td>(nan, nan, nan)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-31 17:01:04</td>\n",
       "      <td>2019-12-31 19:45:05</td>\n",
       "      <td>6</td>\n",
       "      <td>52032.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, nan]</td>\n",
       "      <td>[0.0, nan, nan]</td>\n",
       "      <td>164</td>\n",
       "      <td>[69.0, nan, nan]</td>\n",
       "      <td>(Timestamp('2019-12-31 14:49:22'), NaT, NaT)</td>\n",
       "      <td>(Timestamp('2019-12-31 15:58:02'), NaT, NaT)</td>\n",
       "      <td>(47.22057343, nan, nan)</td>\n",
       "      <td>(-122.3467407, nan, nan)</td>\n",
       "      <td>(58813.0, nan, nan)</td>\n",
       "      <td>58813.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-31 14:49:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-31 15:58:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-31 20:32:41</td>\n",
       "      <td>2020-01-01 01:35:33</td>\n",
       "      <td>3</td>\n",
       "      <td>38063.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[6.0, 0.0]</td>\n",
       "      <td>[6.0, 0.0, nan]</td>\n",
       "      <td>303</td>\n",
       "      <td>[164.0, 69.0, nan]</td>\n",
       "      <td>(Timestamp('2019-12-31 17:01:04'), Timestamp('...</td>\n",
       "      <td>(Timestamp('2019-12-31 19:45:05'), Timestamp('...</td>\n",
       "      <td>(47.518939, 47.22057343, nan)</td>\n",
       "      <td>(-121.84213555, -122.3467407, nan)</td>\n",
       "      <td>(52032.0, 58813.0, nan)</td>\n",
       "      <td>52032.0</td>\n",
       "      <td>58813.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-31 17:01:04</td>\n",
       "      <td>2019-12-31 14:49:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-31 19:45:05</td>\n",
       "      <td>2019-12-31 15:58:02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 02:41:51</td>\n",
       "      <td>2020-01-02 08:52:52</td>\n",
       "      <td>0</td>\n",
       "      <td>4705.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[3.0, 6.0]</td>\n",
       "      <td>[3.0, 6.0, 0.0]</td>\n",
       "      <td>1811</td>\n",
       "      <td>[303.0, 164.0, 69.0]</td>\n",
       "      <td>(Timestamp('2019-12-31 20:32:41'), Timestamp('...</td>\n",
       "      <td>(Timestamp('2020-01-01 01:35:33'), Timestamp('...</td>\n",
       "      <td>(47.8473012, 47.518939, 47.22057343)</td>\n",
       "      <td>(-122.2763971, -121.84213555, -122.3467407)</td>\n",
       "      <td>(38063.0, 52032.0, 58813.0)</td>\n",
       "      <td>38063.0</td>\n",
       "      <td>52032.0</td>\n",
       "      <td>58813.0</td>\n",
       "      <td>2019-12-31 20:32:41</td>\n",
       "      <td>2019-12-31 17:01:04</td>\n",
       "      <td>2019-12-31 14:49:22</td>\n",
       "      <td>2020-01-01 01:35:33</td>\n",
       "      <td>2019-12-31 19:45:05</td>\n",
       "      <td>2019-12-31 15:58:02</td>\n",
       "      <td>303.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02 09:21:03</td>\n",
       "      <td>2020-01-02 14:45:51</td>\n",
       "      <td>1</td>\n",
       "      <td>4705.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 3.0]</td>\n",
       "      <td>[0.0, 3.0, 6.0]</td>\n",
       "      <td>325</td>\n",
       "      <td>[1811.0, 303.0, 164.0]</td>\n",
       "      <td>(Timestamp('2020-01-01 02:41:51'), Timestamp('...</td>\n",
       "      <td>(Timestamp('2020-01-02 08:52:52'), Timestamp('...</td>\n",
       "      <td>(47.22057343, 47.8473012, 47.518939)</td>\n",
       "      <td>(-122.3467407, -122.2763971, -121.84213555)</td>\n",
       "      <td>(4705.0, 38063.0, 52032.0)</td>\n",
       "      <td>4705.0</td>\n",
       "      <td>38063.0</td>\n",
       "      <td>52032.0</td>\n",
       "      <td>2020-01-01 02:41:51</td>\n",
       "      <td>2019-12-31 20:32:41</td>\n",
       "      <td>2019-12-31 17:01:04</td>\n",
       "      <td>2020-01-02 08:52:52</td>\n",
       "      <td>2020-01-01 01:35:33</td>\n",
       "      <td>2019-12-31 19:45:05</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime    leaving_datetime  cluster     dist  last_one  \\\n",
       "0  2019-12-31 14:49:22 2019-12-31 15:58:02        0  58813.0      -1.0   \n",
       "1  2019-12-31 17:01:04 2019-12-31 19:45:05        6  52032.0       0.0   \n",
       "2  2019-12-31 20:32:41 2020-01-01 01:35:33        3  38063.0       6.0   \n",
       "3  2020-01-01 02:41:51 2020-01-02 08:52:52        0   4705.0       3.0   \n",
       "4  2020-01-02 09:21:03 2020-01-02 14:45:51        1   4705.0       0.0   \n",
       "\n",
       "     last_two       last_three  activity_duration  \\\n",
       "0  [nan, nan]  [nan, nan, nan]                 69   \n",
       "1  [0.0, nan]  [0.0, nan, nan]                164   \n",
       "2  [6.0, 0.0]  [6.0, 0.0, nan]                303   \n",
       "3  [3.0, 6.0]  [3.0, 6.0, 0.0]               1811   \n",
       "4  [0.0, 3.0]  [0.0, 3.0, 6.0]                325   \n",
       "\n",
       "  last_three_activity_duration  \\\n",
       "0              [nan, nan, nan]   \n",
       "1             [69.0, nan, nan]   \n",
       "2           [164.0, 69.0, nan]   \n",
       "3         [303.0, 164.0, 69.0]   \n",
       "4       [1811.0, 303.0, 164.0]   \n",
       "\n",
       "                               last_three_start_time  \\\n",
       "0                                    (NaT, NaT, NaT)   \n",
       "1       (Timestamp('2019-12-31 14:49:22'), NaT, NaT)   \n",
       "2  (Timestamp('2019-12-31 17:01:04'), Timestamp('...   \n",
       "3  (Timestamp('2019-12-31 20:32:41'), Timestamp('...   \n",
       "4  (Timestamp('2020-01-01 02:41:51'), Timestamp('...   \n",
       "\n",
       "                                 last_three_end_time  \\\n",
       "0                                    (NaT, NaT, NaT)   \n",
       "1       (Timestamp('2019-12-31 15:58:02'), NaT, NaT)   \n",
       "2  (Timestamp('2019-12-31 19:45:05'), Timestamp('...   \n",
       "3  (Timestamp('2020-01-01 01:35:33'), Timestamp('...   \n",
       "4  (Timestamp('2020-01-02 08:52:52'), Timestamp('...   \n",
       "\n",
       "                         last_three_lat  \\\n",
       "0                       (nan, nan, nan)   \n",
       "1               (47.22057343, nan, nan)   \n",
       "2         (47.518939, 47.22057343, nan)   \n",
       "3  (47.8473012, 47.518939, 47.22057343)   \n",
       "4  (47.22057343, 47.8473012, 47.518939)   \n",
       "\n",
       "                                last_three_lng              last_three_dist  \\\n",
       "0                              (nan, nan, nan)              (nan, nan, nan)   \n",
       "1                     (-122.3467407, nan, nan)          (58813.0, nan, nan)   \n",
       "2           (-121.84213555, -122.3467407, nan)      (52032.0, 58813.0, nan)   \n",
       "3  (-122.2763971, -121.84213555, -122.3467407)  (38063.0, 52032.0, 58813.0)   \n",
       "4  (-122.3467407, -122.2763971, -121.84213555)   (4705.0, 38063.0, 52032.0)   \n",
       "\n",
       "   last_dist  second_last_dist  third_last_dist     last_start_time  \\\n",
       "0        NaN               NaN              NaN                 NaT   \n",
       "1    58813.0               NaN              NaN 2019-12-31 14:49:22   \n",
       "2    52032.0           58813.0              NaN 2019-12-31 17:01:04   \n",
       "3    38063.0           52032.0          58813.0 2019-12-31 20:32:41   \n",
       "4     4705.0           38063.0          52032.0 2020-01-01 02:41:51   \n",
       "\n",
       "  second_last_start_time third_last_start_time        last_end_time  \\\n",
       "0                    NaN                   NaN                  NaN   \n",
       "1                    NaN                   NaN  2019-12-31 15:58:02   \n",
       "2    2019-12-31 14:49:22                   NaN  2019-12-31 19:45:05   \n",
       "3    2019-12-31 17:01:04   2019-12-31 14:49:22  2020-01-01 01:35:33   \n",
       "4    2019-12-31 20:32:41   2019-12-31 17:01:04  2020-01-02 08:52:52   \n",
       "\n",
       "  second_last_end_time  third_last_end_time  last_activity_duration  \\\n",
       "0                  NaN                  NaN                     NaN   \n",
       "1                  NaN                  NaN                    69.0   \n",
       "2  2019-12-31 15:58:02                  NaN                   164.0   \n",
       "3  2019-12-31 19:45:05  2019-12-31 15:58:02                   303.0   \n",
       "4  2020-01-01 01:35:33  2019-12-31 19:45:05                  1811.0   \n",
       "\n",
       "   second_last_activity_duration  third_last_activity_duration  leaving_hour  \\\n",
       "0                            NaN                           NaN            15   \n",
       "1                            NaN                           NaN            19   \n",
       "2                           69.0                           NaN             1   \n",
       "3                          164.0                          69.0             8   \n",
       "4                          303.0                         164.0            14   \n",
       "\n",
       "   day_of_week  is_weekday  third_last_one  second_last_one  \n",
       "0            1           1            -1.0             -1.0  \n",
       "1            1           1            -1.0             -1.0  \n",
       "2            2           1            -1.0              0.0  \n",
       "3            3           1             0.0              6.0  \n",
       "4            3           1             6.0              3.0  "
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this line every time in order to refresh the data\n",
    "# All data sets can be used, but not all are suitable for the kernel\n",
    "df = pd.read_csv('processed/user_0162bdca5925e11a37a48c507453734045b5d62cca0d6abc8300993dfbf8b69e.csv') # 462 observations w/ 21 unique locations (best performance)\n",
    "#df = pd.read_csv('processed/user_0168d97c1ca0723b715aebbc073fc1467a14ce3e7c503f4d6abfd423283292bc.csv') # 340 observations w/ 41 unique locations (poor performance)\n",
    "#df = pd.read_csv('processed/user_0162d8fd57dd8e7ce0ceb9209ab78e5d3619948e70477fbd95d5c746baa564ff.csv') # 333 observations w/ 54 unique locations (poor performance)\n",
    "#df = pd.read_csv('processed/user_01642b85e6773cfbbc3a588c42dffd5db6df684c5965976c7ab0bfd57d7729e8.csv') # 326 observations w/ 85 unique locations (poor performance)\n",
    "#df = pd.read_csv('processed/user_0166568f9c803f70727a6fb01aea245056f51c3d951211d95826508a2a5d0d5c.csv') # 234 observations w/ 60 unique locations (poor performance)\n",
    "#df = pd.read_csv('processed/user_016addadda73ffb46e0b0b6c9b365f995437d01470b782268c87931d0e250c97.csv') # 151 observations w/ 26 unique locations (poor performance)\n",
    "#df = pd.read_csv('processed/user_0169078d983de51eeebfca64093e45ec1d5e73d49eae62239164341560bfe178.csv') # 132 observations w/ 19 unique locations (poor performance)\n",
    "#df = pd.read_csv('processed/user_01676e311de53336910c520f49f9c0cffc949a546885dc14f9c22d170887cb39.csv') # 78 observations w/ 34 unique locations (poor performance)\n",
    "#df = pd.read_csv('processed/user_0167f0fc8edaa27ee9c3d76423c845f164e033625e459d4397fb01898c866d5e.csv') # 63 observations w/ 14 unique locations (poor performance)\n",
    "#df = pd.read_csv('processed/user_01684fd6a43a1f89a8ee2073cdfdb91a8bce5b40a45f25b137d3c7db8c9cc974.csv') # 29 observations w/ 3 unique locations (poor performance)\n",
    "\n",
    "# Pre-process the data\n",
    "# Add additional corrections as needed\n",
    "df['last_one'] = df['last_one'].fillna(-1)\n",
    "df['last_two'] = df['last_two'].apply(lambda x: np.fromstring(x[1 : len(x) - 1], dtype = float, sep = ','))\n",
    "df['last_three'] = df['last_three'].apply(lambda x: np.fromstring(x[1 : len(x) - 1], dtype = float, sep = ','))\n",
    "df['last_three_activity_duration'] = df['last_three_activity_duration'].apply(lambda x: np.fromstring(x[1 : len(x) - 1], dtype = float, sep = ','))\n",
    "df['last_start_time'] = pd.to_datetime(df['last_start_time'])\n",
    "df['leaving_datetime'] = pd.to_datetime(df['leaving_datetime'])\n",
    "\n",
    "# Add additional features\n",
    "df['leaving_hour'] = df['leaving_datetime'].dt.hour.astype(int)\n",
    "df['day_of_week'] = df['leaving_datetime'].dt.dayofweek # Monday = 0, Sunday = 6\n",
    "df['is_weekday'] = df['day_of_week'].apply(lambda x: 1 if x < 5 else 0)\n",
    "\n",
    "# Used for data encoding comparisons\n",
    "df['third_last_one'] = df['last_three'].apply(lambda x: x[2]).fillna(-1)\n",
    "df['second_last_one'] = df['last_two'].apply(lambda x: x[1]).fillna(-1)\n",
    "\n",
    "# View the data\n",
    "pd.options.display.max_columns = None\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes:\n",
    "\n",
    "- Clusters are unnamed locations with cluster 0 as the most frequently visited, decreasing in visitation frequency as cluster value increases. In general, we interpret cluster 0 as home.\n",
    "\n",
    "- The values last_one, last_two, and last_three are sequences in theory, but in application, are tuples that are treated as a single unit. As such, I broke down those values and explore patterns using last_one, second_last_one, and third_last_one as individual location representations. We have empirical evidence that there is better predicitive power when considering the similarity of trip locations as a sequence. For instance, finding similarity between [0, 1, 2] and [0, 1, 3] shows us that we have similarity 2 out of three as opposed to having 0 similarity in terms of a black-and-white approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assist in the analysis of our results, we employ a frequency map. Given every combination of the input features, we tally the number of outputs, thus recording the conditional frequency of the target given the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we wish to learn from\n",
    "features = ['last_one', 'second_last_one', 'third_last_one', 'leaving_hour', 'day_of_week']\n",
    "df['features_hash'] = df[features].apply(lambda x: hash(tuple(x)), axis=1)\n",
    "\n",
    "# create a hashmap which maps each features hash to another hashmap\n",
    "# which maps each cluster to the frequency of that cluster\n",
    "frequency_map = {}\n",
    "for i in range(len(df)):\n",
    "    feature_hash = df['features_hash'][i]\n",
    "    cluster = df['cluster'][i]\n",
    "    if feature_hash not in frequency_map:\n",
    "        frequency_map[feature_hash] = {}\n",
    "        frequency_map[feature_hash][cluster] = 1\n",
    "    else:\n",
    "        if cluster not in frequency_map[feature_hash]:\n",
    "            frequency_map[feature_hash][cluster] = 1\n",
    "        else:\n",
    "            frequency_map[feature_hash][cluster] += 1\n",
    "\n",
    "# uncomment below line if you wish to see the (messy) frequency map \n",
    "# frequency_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we wish to establish a strong baseline to make quality comparisons against. Random Forest classifiers are a common and effective way of accurately predicting classes (in our case, someone's next location). See https://link.springer.com/chapter/10.1007/978-3-030-03146-6_86 for more information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we wish to learn from\n",
    "features = ['last_one', 'second_last_one', 'third_last_one', 'leaving_hour', 'day_of_week']\n",
    "\n",
    "class RandomForest: \n",
    "    \"\"\"\n",
    "    A majority vote random forest classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_trees, max_depth=None):\n",
    "        \"\"\"\n",
    "        Constructs a RandomForest that uses the given number of trees, each with a \n",
    "        max depth of max_depth.\n",
    "        \"\"\"\n",
    "        self._trees = [\n",
    "            DecisionTreeClassifier(max_depth=max_depth, random_state=1) \n",
    "            for i in range(num_trees)\n",
    "        ]\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Takes an input dataset X and trains each tree on a random subset of the data.\n",
    "        Sampling is done with replacement.\n",
    "        \"\"\"\n",
    "\n",
    "        for mini_tree in self._trees:\n",
    "            mini_data = X.iloc[np.random.randint(0, X.shape[0], X.shape[0])]\n",
    "            mini_tree.fit(mini_data[features], mini_data['cluster'])\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Takes an input dataset X and returns the predictions for each example in X.\n",
    "        \"\"\"\n",
    "        # Builds up a 2d array with n rows and T columns\n",
    "        # where n is the number of points to classify and T is the number of trees\n",
    "        predictions = np.zeros((len(X), len(self._trees)))\n",
    "        for i, tree in enumerate(self._trees):\n",
    "            # Make predictions using the current tree\n",
    "            preds = tree.predict(X)\n",
    "            \n",
    "            # Store those predictions in ith column of the 2d array\n",
    "            predictions[:, i] = preds\n",
    "            \n",
    "        # For each row of predictions, find the most frequent label (axis=1 means across columns)\n",
    "        return scipy.stats.mode(predictions, axis=1, keepdims=False)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data\n",
    "train_set, test_set = train_test_split(df, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Get x data\n",
    "x_train = train_set[features].values\n",
    "x_test = test_set[features].values\n",
    "\n",
    "# Convert cluster column to tensor\n",
    "y_train = train_set['cluster'].values\n",
    "y_actual = test_set['cluster'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test Accuracy: 0.9247311827956989\n",
      "Best Test Depth: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          2,   0,   0,   8,   0,  -1,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  -3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0, -10,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Error: 26\n"
     ]
    }
   ],
   "source": [
    "# First calculate the accuracies for each depth\n",
    "depths = list(range(1, 26, 2))\n",
    "best_depth = 0\n",
    "best_accuracy = 0\n",
    "best_pred = None\n",
    "\n",
    "for i in depths:\n",
    "    # Train and evaluate our RandomForest classifier with given max_depth \n",
    "    forest = RandomForest(15, max_depth=i)\n",
    "    forest.fit(train_set)\n",
    "    train_score = accuracy_score(forest.predict(train_set[features]), y_train)\n",
    "    observed_pred = forest.predict(test_set[features])\n",
    "    test_score = accuracy_score(observed_pred, y_actual)\n",
    "    \n",
    "    if test_score > best_accuracy:\n",
    "        best_accuracy = test_score\n",
    "        best_depth = i\n",
    "        best_pred = observed_pred\n",
    "\n",
    "print(\"Best Test Accuracy: \", best_accuracy, \"\\nBest Test Depth: \", best_depth, sep=\"\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    display(torch.tensor(y_actual - best_pred, dtype=torch.int))\n",
    "    print(\"Total Error: \", '%d'%((torch.sum(torch.abs(torch.tensor(y_actual - best_pred, dtype=torch.int))))), sep=\"\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze our baseline, we use the frequency map to help determine how far off our prediction was from the base result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   2.,   0.,   0.,   8.,   0.,  -1.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  -3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual - best_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(index: 14) (y_actual: 2) (RF_pred: 0) (actual - predicted frequency: -2)\n",
      "(index: 17) (y_actual: 10) (RF_pred: 2) (actual - predicted frequency: 1)\n",
      "(index: 19) (y_actual: 0) (RF_pred: 1) (actual - predicted frequency: 1)\n",
      "(index: 26) (y_actual: 1) (RF_pred: 0) (actual - predicted frequency: 1)\n",
      "(index: 45) (y_actual: 1) (RF_pred: 4) (actual - predicted frequency: 1)\n",
      "(index: 73) (y_actual: 2) (RF_pred: 1) (actual - predicted frequency: -7)\n",
      "(index: 81) (y_actual: 0) (RF_pred: 10) (actual - predicted frequency: 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_actual)):\n",
    "    if y_actual[i] - best_pred[i] != 0:\n",
    "        actual_freq = frequency_map[hash(tuple(x_test[i]))][y_actual[i]]\n",
    "        \n",
    "        # We must also account for if/when best_pred[i] is not a cluster label.\n",
    "        # For instance, the hashcode at index 17 has no mapping to best_pred[17] = 2.\n",
    "        # As such, we will prevent the code from running and instead set the value to 0.\n",
    "        if best_pred[i].astype(int) not in frequency_map[hash(tuple(x_test[i]))]:\n",
    "            pred_freq = 0\n",
    "        else:\n",
    "            pred_freq = frequency_map[hash(tuple(x_test[i]))][best_pred[i].astype(int)]\n",
    "        print(\"(index: \", i, \") (y_actual: \", y_actual[i], \") (RF_pred: \", best_pred[i].astype(int), \") (actual - predicted frequency: \", actual_freq - pred_freq, \")\", sep=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the random forest classifier does a strong job of keeping the actual and predicted frequencies fairly close. Even though some locations were inaccurately predicted, we were still choosing locations that were almost as frequent given the specific combination of input features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet GP Classification Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dirichlet classification is a useful method for GP classification problems. In general, GP classification methods are expensive to run, which hinder many applications to large-scale multi-class classification problems. However, the Dirichlet GP classification method allows us to perform regression on the labels which \"leads to fast training and excellent classification accuracies... without compromising on calibration\" (Milios et. al., 2018). The theory can be explored in this article: https://papers.nips.cc/paper/2018/file/b6617980ce90f637e68c3ebe8b9be745-Paper.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Dirichlet Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We naively implement Dirichlet classification on five input features with the hope of accurately predicting our participant's next location. We first set up our data and create our train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: torch.Size([369, 5])\n",
      "testing shape: torch.Size([93, 5])\n"
     ]
    }
   ],
   "source": [
    "input_features = ['last_one', 'second_last_one', 'third_last_one', 'leaving_hour', 'day_of_week']\n",
    "\n",
    "# Split data into standard 80/20 train/test split\n",
    "train_set, test_set = train_test_split(df, test_size = 0.2, random_state=42)\n",
    "\n",
    "# x_train is last columns of train_set\n",
    "x_train = torch.tensor(train_set[input_features].values, dtype=torch.int64)\n",
    "print(\"training shape:\", x_train.shape)\n",
    "\n",
    "# x_test is last columns of test_set\n",
    "x_test = torch.tensor(test_set[input_features].values, dtype=torch.int64)\n",
    "print(\"testing shape:\", x_test.shape)\n",
    "\n",
    "# Convert cluster column to tensor\n",
    "y_train = torch.tensor(train_set['cluster'].values, dtype=torch.int64)\n",
    "y_actual = torch.tensor(test_set['cluster'].values, dtype=torch.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implement a basic Dirichlet model. GPyTorch documentation can be found here: https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/GP_Regression_on_Classification_Labels.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "# we let the DirichletClassificationLikelihood compute the targets for us\n",
    "likelihood = DirichletClassificationLikelihood(y_train, learn_additional_noise=True)\n",
    "model = DirichletGPModel(x_train, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our model and optimize our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 82.225\n",
      "Iter 6/50 - Loss: 63.065\n",
      "Iter 11/50 - Loss: 52.981\n",
      "Iter 16/50 - Loss: 48.169\n",
      "Iter 21/50 - Loss: 45.558\n",
      "Iter 26/50 - Loss: 43.880\n",
      "Iter 31/50 - Loss: 42.677\n",
      "Iter 36/50 - Loss: 41.776\n",
      "Iter 41/50 - Loss: 41.097\n",
      "Iter 46/50 - Loss: 40.586\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(x_train)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "    loss.backward()\n",
    "    if i % 5 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate/Test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "    test_dist = model(x_test)\n",
    "    pred_means = test_dist.loc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,   0,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          2,   0,   0,   7,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  -1,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  -1,   0,   0,   0,   0,   0,   0,   3, -13,   0,   0,   0,   0,\n",
       "          0,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observed_prediction = pred_means.max(0)[1]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    display(torch.tensor(y_actual - observed_prediction, dtype=torch.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict the next location (current cluster) with an accuracy of: 89.25%\n",
      "Total Error: 32\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "      warnings.simplefilter(\"ignore\")\n",
    "      print(\"We predict the next location (current cluster) with an accuracy of: \", \n",
    "            '%.2f'%((len(y_actual) - torch.count_nonzero(torch.tensor(y_actual - observed_prediction, dtype=torch.int))) / len(y_actual) * 100), \"%\", sep=\"\")\n",
    "      print(\"Total Error: \", '%d'%((torch.sum(torch.abs(torch.tensor(y_actual - observed_prediction, dtype=torch.int))))), sep=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "Here we see that our prediction accuracy is fairly good, accurately predicting 80/93 locations exactly. However, we can improve our results! Notice how our inputs are all discrete data types (last locations, hour of day, and day of week). We hypothesize that a better result could be gained if we treat our last locations as simply labels (using one-hot encoding) rather than weighted integers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Sequences "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding upon the previous model's discussion, we implement one-hot encoding as a way into our location variables as a way to transform our weighted locations into labels. We then append the leaving hour of day and day of week manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: torch.Size([369, 68])\n",
      "testing shape: torch.Size([93, 68])\n"
     ]
    }
   ],
   "source": [
    "dummies1 = pd.get_dummies(df['third_last_one']).values\n",
    "dummies2 = pd.get_dummies(df['second_last_one']).values\n",
    "dummies3 = pd.get_dummies(df['last_one']).values\n",
    "\n",
    "# May be used for experiments, but does not improve the results\n",
    "dummies4 = pd.get_dummies(df['leaving_hour']).values\n",
    "dummies5 = pd.get_dummies(df['day_of_week']).values\n",
    "\n",
    "# connect old data fram with new one horizontally\n",
    "new_df = pd.concat([df, \n",
    "                    pd.DataFrame(dummies1),\n",
    "                    pd.DataFrame(dummies2),\n",
    "                    pd.DataFrame(dummies3), \n",
    "                    pd.DataFrame(df['leaving_hour']),\n",
    "                    pd.DataFrame(df['day_of_week'])], axis=1)\n",
    "train_set, test_set = train_test_split(new_df, test_size = 0.2, random_state=42)\n",
    "\n",
    "# The length of our input tensor\n",
    "input_length = dummies1.shape[1] + dummies2.shape[1] + dummies3.shape[1]\n",
    "\n",
    "# x_train is last input_length columns of train_set + 2 columns for hour and day\n",
    "x_train = torch.tensor(train_set.iloc[:, -(input_length + 2):].values, dtype=torch.int64)\n",
    "print(\"training shape:\", x_train.shape)\n",
    "\n",
    "# x_test is last input_length columns of test_set + 2 columns for hour and day\n",
    "x_test = torch.tensor(test_set.iloc[:, -(input_length + 2):].values, dtype=torch.int64)\n",
    "print(\"testing shape:\", x_test.shape)\n",
    "\n",
    "# Convert cluster column to tensor\n",
    "y_train = torch.tensor(train_set['cluster'].values, dtype=torch.int64)\n",
    "y_actual = torch.tensor(test_set['cluster'].values, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "# we let the DirichletClassificationLikelihood compute the targets for us\n",
    "likelihood = DirichletClassificationLikelihood(y_train, learn_additional_noise=True)\n",
    "model = DirichletGPModel(x_train, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 88.325\n",
      "Iter 11/50 - Loss: 52.427\n",
      "Iter 21/50 - Loss: 43.415\n",
      "Iter 31/50 - Loss: 41.038\n",
      "Iter 41/50 - Loss: 39.949\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train)\n",
    "    loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "    loss.backward()\n",
    "    if i % 10 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our model\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "    test_dist = model(x_test)\n",
    "    pred_means = test_dist.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  5,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -6,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observed_prediction = pred_means.max(0)[1]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    display(torch.tensor(y_actual - observed_prediction, dtype=torch.int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the difference matrix above, we interpret this as:\n",
    "- 0's being correct predictions\n",
    "- Positive values being undershot predictions (chose something less frequent)\n",
    "- Negative values being overshot predictions (chose something more frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predict the next location (current cluster) with an accuracy of: 94.62%\n",
      "Total Error: 16\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "      warnings.simplefilter(\"ignore\")\n",
    "      print(\"We predict the next location (current cluster) with an accuracy of: \", \n",
    "      '%.2f'%((len(y_actual) - torch.count_nonzero(torch.tensor(y_actual - observed_prediction, dtype=torch.int))) / len(y_actual) * 100), \"%\", sep=\"\")\n",
    "      print(\"Total Error: \", '%d'%((torch.sum(torch.abs(torch.tensor(y_actual - observed_prediction, dtype=torch.int))))), sep=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze our GP model, we use the frequency map to help determine how far off our prediction was from the actual result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  5,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -6,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0])"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual - observed_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(index: 4) (y_actual: 2) (GP_pred: 0) (actual - predicted frequency: 2)\n",
      "(index: 14) (y_actual: 2) (GP_pred: 0) (actual - predicted frequency: -2)\n",
      "(index: 17) (y_actual: 10) (GP_pred: 5) (actual - predicted frequency: 1)\n",
      "(index: 52) (y_actual: 3) (GP_pred: 9) (actual - predicted frequency: 1)\n",
      "(index: 73) (y_actual: 2) (GP_pred: 1) (actual - predicted frequency: -7)\n"
     ]
    }
   ],
   "source": [
    "# Reset x_test to its original values\n",
    "hashcodes = test_set['features_hash'].values\n",
    "\n",
    "for i in range(len(y_actual)):\n",
    "    actual = y_actual[i].int().item()\n",
    "    predicted = observed_prediction[i].int().item()\n",
    "    if actual - predicted != 0:\n",
    "        actual_freq = frequency_map[hashcodes[i]][actual]\n",
    "        \n",
    "        # We must also account for if/when best_pred[i] is not a cluster label.\n",
    "        # For instance, the hashcode at index 17 has no mapping to best_pred[17] = 2.\n",
    "        # As such, we will prevent the code from running and instead set the value to 0.\n",
    "        if predicted not in frequency_map[hashcodes[i]]:\n",
    "            pred_freq = 0\n",
    "        else:\n",
    "            pred_freq = frequency_map[hashcodes[i]][predicted]\n",
    "        print(\"(index: \", i, \") (y_actual: \", actual, \") (GP_pred: \", predicted, \") (actual - predicted frequency: \", actual_freq - pred_freq, \")\", sep=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After changing the encoding of our data, we can see a massive increase in our Dirichlet classification accuracy, and a stronger performance than random forest. If you're curious on how random forest performs under this data encoding, you can run the model prediction above. You will find that there is no significant difference between the encodings in terms of prediction accuracy and total error.\n",
    "\n",
    "Dirichlet classification is very consistent, providing the same scores and same error every iteration. Random Forest, on the other hand, varies in total error, but tends to stay consistent with its total prediction accuracy. We also saw from the frequency table that Random Forest and Dirichlet both have very similar frequency errors.\n",
    "\n",
    "Additionally, there may be questions regarding why we didn't one-hot encode all five values, and only the locations, when all of them are discrete data types. While time and day can be viewed as continuous variables, we wish for our locations to be treated as labels and not necessarily have numerical weight tied to them. This is because the clusters are arranged by frequency, but the frequency is not necessarily scale predictably with the cluster value. Empirically, this can be confirmed by testing it out above. You will find that the model performs worse using one-hot encodings of time and day.\n",
    "\n",
    "In our opinion Gaussian Processes are strong condtendors for predictive capabilities within the realm of trajectory prediction. Future works may include combinations with other machine learning techniques to boost performance, additional measures for analyzing accuracy, and travel time prediction (in conjunction with location prediction)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPs are popular in travel time prediction, but to our knowledge, have not yet been employed for travel trajectory and location prediction.\n",
    "\n",
    "Note: Focus on explanation for our error and possibilities of inaccuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
